{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d8f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceManager] using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "from manager.device_manager import DeviceManager\n",
    "from model.model import GPT, GPTConfig, GPTSeparateAttention\n",
    "from config import Config\n",
    "\n",
    "dm = DeviceManager()\n",
    "model_dir = 'logs/model_19072.pt'\n",
    "\n",
    "# Load tiktoken encoder\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(model_dir, map_location=dm.device, weights_only=False)\n",
    "model_config = checkpoint[\"config\"]\n",
    "model = GPT(model_config)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "model.to(dm.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6ca8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTSeparateAttention(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x BlockSeparate(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttentionSeparate(\n",
       "          (c_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_model = GPTSeparateAttention.from_gpt(model)\n",
    "separate_model.to(dm.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94de529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Once upon a time, there was a man named John D. Dufield; at that time, John P. McFarland was called \"Hugh McFarland.\" His nephews would travel on them, and when they were all taken to America, they had three daughters, John, Mary, and Mary.\n",
      "These were three sons. John is the oldest of two children; Mary, whose children are as follows: her oldest daughter; her second oldest son, John; and her grandson, William. At least three young people grew up to be children of the King, John and Mary. Many of their families did not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "print(\"Generating text...\")\n",
    "separate_model.sample_sequence(\n",
    "\t\"Once upon a time, there was a man\",\n",
    "\tdm,\n",
    "\tenc,\n",
    "\t1,\n",
    "\t128,\n",
    "\ttop_priority=50,\n",
    ")  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
